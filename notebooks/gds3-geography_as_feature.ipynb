{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geography as Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import libpysal as lp\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import contextily as ctx\n",
    "import shapely.geometry as geom\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll talk about representing spatial relationships in Python using PySAL's *spatial weights* functionality. This provides a unified way to express the spatial relationships between observations. \n",
    "\n",
    "First, though, we'll need to read in our data built in the `relations.ipynb` notebook: Airbnb listings & nightly prices for neighbourhoods in Austin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = gpd.read_file('../data/listings.gpkg').to_crs(epsg=3857)\n",
    "neighborhoods = gpd.read_file('../data/neighborhoods.gpkg').to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we'll grab a basemap for our study area using `contextily`. Contextily is package designed to provide basemaps for data. It's best used for data in webmercator or raw WGS longitude-latitude coordinates.\n",
    "\n",
    "Below, we are going to grab the basemap images for the `total_bounds` of our study area at a given zoom level. Further, we are specifying a different tile server from the default, the [Stamen Maps `toner-lite` tiles](http://maps.stamen.com/m2i/#toner-lite/1500:1000/12/47.5462/7.6196), to use since we like its aesthetics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemap, bounds = ctx.bounds2img(*listings.total_bounds, zoom=10, \n",
    "                                 url=ctx.tile_providers.ST_TONER_LITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatial plotting has come a long way since we first started in spatial data science. But, a few tricks for `geopandas` are still somewhat arcane, so it's useful to know them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8,8))\n",
    "ax = plt.gca()\n",
    "# TRICK 1: when you only want to plot the boundaries, not the polygons themselves:\n",
    "neighborhoods.boundary.plot(color='k', ax=ax)\n",
    "ax.imshow(basemap, extent=bounds, interpolation='bilinear')\n",
    "ax.axis(neighborhoods.total_bounds[np.asarray([0,2,1,3])])\n",
    "# TRICK 2: Sorting the data before plotting it will ensure that \n",
    "#          the highest (or lowest) categories are prioritized in the plot.\n",
    "#          Use this to mimick blending or control the order in which alpha blending might occur. \n",
    "listings.sort_values('price').plot('price', ax=ax, marker='o', cmap='plasma', alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Weights: expressing spatial relationships mathematically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatial weights matrices are mathematical objects that are designed to express the inter-relationships between sites in a given geolocated frame of analysis. \n",
    "\n",
    "This means that the relationships between each site (of which there are usually $N$) to every other site is *represented* by the weights matrix, which is some $N \\times N$ matrix of \"weights,\" which are scalar numerical representations of these relationships.\n",
    "In a similar fashion to *affinity matrices* in machine learning, spatial weights matrices are used in a wide variety of problems and models in quantitative geography and spatial data science to express the spatial relationships present in our data. \n",
    "\n",
    "In python, PySAL's `W` class is the main method by which people construct & represent spatial weights. This means that arbitary inter-site linkages can be expressed using one dictionary, and another *optional* dictionary: \n",
    "\n",
    "- **a `neighbors` dictionary,** which encodes a *focal observation*'s \"name\" and which other \"named\" observations the focal is linked.\n",
    "- **a `weights` dictionary,** which encodes how strongly each of the neighbors are linked to the focal observation. \n",
    "\n",
    "Usually, these are one-to-many mappings, dictionaries keyed with the \"focal\" observation and values which are lists of the names to which the key is attached.\n",
    "\n",
    "An example below shows three observations, `a`,`b`, and `c`, arranged in a straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = dict(a = ['b'],\n",
    "                 b = ['a','c'],\n",
    "                 c = ['b']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connectivity strength is recorded in a separate dictionary whose keys should align with the `neighbors`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dict(a = [1],\n",
    "               b = [.2, .8],\n",
    "               c = [.3]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the most generic spatial weights object, only the `neighbors` dictionary is required; the `weights` will assumed to be one everywhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = lp.weights.W(neighbors) # assumes all weights are one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = lp.weights.W(neighbors, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing different types of weights\n",
    "\n",
    "By itself, this is not really useful; the hardest part of *using* these representations is constructing them from your original spatial data. Thus, we show below how this can be done. First, we cover *contiguity* weights, which are analogues to adjacency matrices . These are nearly always used for polygonal \"lattice\" data, but can also be used for points as well by examining their voronoi diagram. \n",
    "\n",
    "Second, we cover *distance* weights, which usually pertain to point data only. These tend to embed notions of distance decay, and are incredibly flexible for multiple forms of spatial data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contiguity\n",
    "\n",
    "\n",
    "Contiguity weights, or \"adjacency matrices,\" are one common representation of spatial relationships that spring to mind when modeling how polygons relate to one another. In this representation, objects are considered \"near\" when they touch, and \"far\" when they don't. adjacency is considered as a \"binary\" relationship, so all polygons that are near to one another are *as near as they are to any other near polygon*. \n",
    "\n",
    "We've got fast algos to build these kinds of relationships from `shapely`/`geopandas`, as well as directly from files (without having to read all the data in at once). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qneighbs = lp.weights.Queen.from_dataframe(neighborhoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pysal` library has gone under a bit of restructuring. \n",
    "\n",
    "The main components of the package are migrated to `libpysal`, which forms the base of a constellation of spatial data science packages. \n",
    "\n",
    "\n",
    "Given this, we you can plot the adjacency graph for the polygons we showed above as another layer in the plot. We will remove some of the view to make the view simpler to examine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8,8))\n",
    "ax = plt.gca()\n",
    "# when you only want to plot the boundaries:\n",
    "neighborhoods.boundary.plot(color='k', ax=ax, alpha=.4)\n",
    "Qneighbs.plot(neighborhoods, edge_kws=dict(linewidth=1.5, color='orangered'), \n",
    "              node_kws=dict(marker='*'), ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can check if individual observations are disconnected using the weights object's `islands` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qneighbs.islands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good news, as each polygon has at least one neighbor, and our graph has a single connected component.\n",
    "\n",
    "PySAL weights can be used in other packages by converting them into their equivalent matrix representations. Sparse and dense array versions are offered, with `.sparse` providing the sparse matrix representation, and `.full()` providing the ids and dense matrix representing the graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spqneighbs = Qneighbs.sparse\n",
    "spqneighbs.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the matrix, you can see that the adjacency matrix is very sparse indeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(spqneighbs.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the number of links as a percentage of all possible $N^2$ links from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qneighbs.pct_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means that there are around 12.3% of all the possible connections between any two observations actually make it into the adjacency graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For contiguity matrices, this only has binary elements, recording 1 where two observations are linked. Everywhere else, the array is empty (zero, in a dense representation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(spqneighbs.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately for us, PySAL plays real well with scipy & other things built on top of SciPy. So, the [new compressed sparse graph (`csgraph`)](https://docs.scipy.org/doc/scipy/reference/sparse.csgraph.html) module in SciPy works wonders with the PySAL sparse weights representations. So, we often will jump back and forth between PySAL weights and scipy tools when working with these spatial representations of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse.csgraph as csgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in `csgraph`, there are a ton of tools to work with graphs. For example, we could use `csgraph.connected_components`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_connected, labels = csgraph.connected_components(spqneighbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verify that we have a single connected component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(number_connected, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qconnected = lp.weights.Queen.from_dataframe(neighborhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qconnected.plot(neighborhoods, node_kws=dict(marker='*'), edge_kws=dict(linewidth=.4))\n",
    "neighborhoods.boundary.plot(color='r', ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we could use the `lp.w_subset` function, which would avoid re-constructing the weights again. This might help if they are truly massive, but it's often just as expensive to discover the subset as it is to construct a new weights object from this subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qconnected2 = lp.weights.w_subset(Qneighbs, ids=[i for i in range(Qneighbs.n) if labels[i] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, if `pandas` rearranges the dataframes, these will appear to be different weights since the ordering is different. To check if two weights objects are identical, a simple test is to check the sparse matrices for **in**equality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Qconnected2.sparse != Qconnected.sparse).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Representations\n",
    "\n",
    "PySAL, by default, tends to focus on a single `W` object, which provides easy tools to construct & work with the accompanying sparse matrix representations. \n",
    "\n",
    "However, it's often the case we want alternative representations of the same relationships. \n",
    "\n",
    "One handy one is the weights list. This is an alternative form of expressing a weights matrix, and provides a copy of the underlying `W.sparse.data`, made more regular and put into a pandas dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist = Qconnected.to_adjlist()\n",
    "adjlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This handy if you'd rather work with the representation in terms of individual edges, rather than in sets of edges. \n",
    "\n",
    "Also, it is exceptionally handy when you want to ask questions about the data used to generate the spatial weights, since it lets you attach this data to each of the focal pairs and ask questions about the associated data at that level. \n",
    "\n",
    "For example, say we get the median price of airbnbs within a given neighbourhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.price.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = listings[['price']].replace('[\\$,]', '', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.mean(), price.max(), price.median(), price.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings['price'] = price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to attach that back to the dataframe containing the neighbourhood information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_prices = gpd.sjoin(listings[['price', 'geometry']], neighborhoods, op='within')\\\n",
    "                   .groupby('index_right').price.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods = neighborhoods.merge(median_prices.to_frame('median_price'), \n",
    "                                    left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can map this information at the neighbourhood level, computed from the individual listings within each neighbourhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8,8))\n",
    "ax = plt.gca()\n",
    "# when you only want to plot the boundaries:\n",
    "neighborhoods.plot('median_price', cmap='plasma', alpha=.7, ax=ax)\n",
    "#basemap of the area\n",
    "ax.imshow(basemap, extent=bounds, interpolation='gaussian')\n",
    "ax.axis(neighborhoods.total_bounds[np.asarray([0,2,1,3])])\n",
    "#if you want the highest values to show on top of lower ones\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to examine the local relationships in price between nearby places, we could merge this information back up with the weights list and get the difference in price between every adjacent neighbourhood. \n",
    "\n",
    "Usually, these joins involve building links between both the focal and neighbor observation IDs. You can do this simply by piping together two merges: one that focuses on the \"focal\" index and one that focuses on the \"neighbor\" index.\n",
    "\n",
    "Using a suffix in the later merge will give the data joined on the focal index a distinct name from that joined on the neighbor index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist = adjlist.merge(neighborhoods[['hood_id', \n",
    "                                        'median_price']], \n",
    "                        left_on='focal', right_index=True, how='left')\\\n",
    "                  .merge(neighborhoods[['hood_id', \n",
    "                                        'median_price']], \n",
    "                         left_on='neighbor', right_index=True ,how='left', \n",
    "                         suffixes=('_focal', '_neighbor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist.median_price_neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can group by the `focal` index and take the difference of the prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricediff = adjlist[['median_price_focal', \n",
    "                     'median_price_neighbor']].diff(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricediff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can link this back up to the original adjacency list, but first let's rename the column we want to `price_difference` and only keep that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricediff['price_difference'] = pricediff[['median_price_neighbor']]\n",
    "adjlist['price_difference'] = pricediff[['price_difference']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, if we wanted to find the pair of adjacent neighbourhoods with the greatest price difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can group by *both* the focal and neighbor name to get a meaningful list of all the neighborhood boundaries & their difference in median listing price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts = adjlist.groupby((\"hood_id_focal\", \"hood_id_neighbor\"))\\\n",
    "                   .price_difference.median().abs()\\\n",
    "                   .sort_values().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For about six neighbourhood pairs (since these will be duplicate `(A,B) & (B,A)` links), the median listing price is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts.query('price_difference == 0').sort_values(['hood_id_focal','hood_id_neighbor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other end, the 20 largest paired differences in median price between adjacent neighbourhoods is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrasts.sort_values(['price_difference',\n",
    "                       'hood_id_focal'],\n",
    "                       ascending=[False,True]).head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contiguity for points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contiguity can also make sense for point objects as well, if you think about the corresponding Voronoi Diagram and the Thiessen Polygons's adjacency graph. \n",
    "\n",
    "Effectively, this connects each point to a set of its nearest neighbouring points, without pre-specifying the number of points.\n",
    "\n",
    "We can use it to define relationships between airbnb listings in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.sort_values('price').plot('price', cmap='plasma', alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libpysal.cg.voronoi import voronoi_frames\n",
    "from libpysal.weights import Voronoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.cg.voronoi_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.weights.Voronoi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = np.vstack((listings.centroid.x, listings.centroid.y)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thiessens, points = voronoi_frames(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the \"natural\" polygons generated by the `scipy.distance.voronoi` object may be excessively big, since some of the nearly-parallel lines in the voronoi diagram may take a long time to intersect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(2.16*4,4))\n",
    "thiessens.plot(ax=ax[0], edgecolor='k')\n",
    "neighborhoods.plot(ax=ax[0], color='w', edgecolor='k')\n",
    "ax[0].axis(neighborhoods.total_bounds[np.asarray([0,2,1,3])])\n",
    "ax[0].set_title(\"Where we want to work\")\n",
    "thiessens.plot(ax=ax[1])\n",
    "neighborhoods.plot(ax=ax[1], color='w', edgecolor='k')\n",
    "ax[1].set_title(\"The outer limit of the voronoi diagram from SciPy\")\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, PySAL can work with this amount of observations to build weights really quickly. But, the `geopandas` overlay operation is very slow for this many polygons, so even with a spatial index, clipping these polygons to the bounding box can take a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thiessens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods['dummy']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've precomputed the clipped version of the thiessen polygons and stored them, so that we can move forward without waiting too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper = neighborhoods.dissolve(by='dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thiessens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thiessens.crs = clipper.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_thiessens = gpd.overlay(thiessens, clipper, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_thiessens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_thiessens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_thiessens.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_thiessens.to_file('../data/thiessens.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_thiessens = gpd.read_file('../data/thiessens.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, whereas the overlay operation to clean up this diagram took quite a bit of computation time if just called regularly ([and there may be plenty faster ways to do these kinds of ops](http://2018.geopython.net/#w4)), constructing the topology for all 11k Thiessen polygons is rather fast:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to show what this looks like, we will plot a part of one of the neighbourhoods in Austin: Hyde Park to the North of UT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_neighborhood = 'Hyde Park'\n",
    "focal = clipped_thiessens[listings.hood == focal_neighborhood]\n",
    "focal = focal.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thiessen_focal_w = lp.weights.Rook.from_dataframe(focal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,3,figsize=(15,5),sharex=True,sharey=True)\n",
    "\n",
    "# plot the airbnbs across the map\n",
    "\n",
    "listings.plot('price', cmap='plasma', ax=ax[0],zorder=0, marker='.')\n",
    "# \n",
    "ax[0].set_xlim(*focal.total_bounds[np.asarray([0,2])])\n",
    "ax[0].set_ylim(*focal.total_bounds[np.asarray([1,3])])\n",
    "# Plot the thiessens corresponding to each listing in focal neighbourhood\n",
    "listings[listings.hood == focal_neighborhood]\\\n",
    "        .plot('price', cmap='plasma', marker='.', ax=ax[1], zorder=0)\n",
    "focal.boundary.plot(ax=ax[1], linewidth=.7)\n",
    "    \n",
    "thiessen_focal_w.plot(focal, node_kws=dict(marker='.',s=0), \n",
    "                      edge_kws=dict(linewidth=.5), color='b', ax=ax[2])\n",
    "focal.boundary.plot(ax=ax[2], linewidth=.7)\n",
    "\n",
    "\n",
    "# underlay the neighbourhood boundaries\n",
    "for ax_ in ax:\n",
    "    neighborhoods.boundary.plot(ax=ax_, color='grey',zorder=1)\n",
    "    ax_.set_xticklabels([])\n",
    "    ax_.set_yticklabels([])\n",
    "ax[0].set_title(\"All Listings\", fontsize=20)\n",
    "ax[1].set_title(\"Voronoi for Listings in %s\"%focal_neighborhood, fontsize=20)\n",
    "ax[2].set_title(\"AdjGraph for Listings Voronoi\", fontsize=20)\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance weights tend to reflect relationships that work based on distance decay. Often, people think of spatial kernel functions when talking about distance weighting. But, PySAL also recognizes/uses distance-banded weights, which consider any neighbor within a given distance threshold as \"near,\" and K-nearest neighbor weights, which consider any of the $k$-closest points to each point as \"near\" to that point. \n",
    "\n",
    "KNN weights, by default, are the only asymmetric weight PySAL will construct. However, using `csgraph`, one could prune/trim any of the contiguity or distance weights to be directed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights are one of the most commonly-used kinds of distance weights. They reflect the case where similarity/spatial proximity is assumed or expected to decay with distance.\n",
    "\n",
    "Many of these are quite a bit more heavy to compute than the contiguity graph discussed above, since the contiguity graph structure embeds simple assumptions about how shapes relate in space that kernel functions cannot assume. \n",
    "\n",
    "Thus, I'll subset the data to a specific area of Austin before proceeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings['hood']=listings['hood'].fillna(value=\"None\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_listings = listings[listings.hood.str.startswith(\"Hyde\")].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_listings.sort_values('price').plot('price', cmap='plasma', zorder=3)\n",
    "neighborhoods.boundary.plot(color='grey', ax=plt.gca())\n",
    "plt.axis(focal_listings.total_bounds[np.asarray([0,2,1,3])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wkernel = lp.weights.Kernel.from_dataframe(focal_listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you wanted to see what these look like on the map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_listings.assign(weights=Wkernel.sparse[0,:].toarray().flatten()).plot('weights', cmap='plasma')\n",
    "neighborhoods.boundary.plot(color='grey', ax=plt.gca())\n",
    "plt.axis(focal_listings.total_bounds[np.asarray([0,2,1,3])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, clearly, near things are weighted very highly, and distant things are weighted low. \n",
    "\n",
    "So, if you're savvy with this, you may wonder:\n",
    "> Why use PySAL kernel weights when `sklearn.pairwise.kernel_metrics` are so much faster?\n",
    "\n",
    "Well, PySAL's got a few enhancements over and above scikit kernel functions. \n",
    "1. **pre-specified bandwidths**: using the `bandwidth=` argument, you can give a specific bandwidth value for the kernel weight. This lets you use them in optimization routines where bandwidth might need to be a parameter that's optimized by another function.\n",
    "2. **fixed vs. adaptive bandwidths**: adaptive bandwidths adjust the map distanace to make things more \"local\" in densely-populated areas of the map and less \"local\" in sparsely-populated areas. This is adjusted by the...\n",
    "3. **`k`-nearest neighborhood tuning**: this argument adjusts the number of nearby observations to use for the bandwidth. \n",
    "\n",
    "Also, many of the scikit kernel functions are also implemented. The default is the `triangular` weight, which is a linear decay with distance.\n",
    "\n",
    "For example, an adaptive Triangular kernel and an adaptive Gaussian kernel are shown below, alongisde the same point above for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wkernel_adaptive = lp.weights.Kernel.from_dataframe(focal_listings, k=20, fixed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wkernel_adaptive_gaussian = lp.weights.Kernel.from_dataframe(focal_listings, k=10, fixed=False, function='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,3,figsize=(12,4))\n",
    "focal_listings.assign(weights=Wkernel.sparse[0,:].toarray().flatten()).plot('weights', cmap='plasma',ax=ax[0])\n",
    "focal_listings.assign(weights=Wkernel_adaptive.sparse[0,:].toarray().flatten()).plot('weights', cmap='plasma',ax=ax[1])\n",
    "focal_listings.assign(weights=Wkernel_adaptive_gaussian.sparse[0,:].toarray().flatten()).plot('weights', cmap='plasma',ax=ax[2])\n",
    "for i in range(3):\n",
    "    neighborhoods.boundary.plot(color='grey', ax=ax[i])\n",
    "    ax[i].axis(focal_listings.total_bounds[np.asarray([0,2,1,3])])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticklabels([])\n",
    "ax[0].set_title(\"Defaults (Triangular fixed kernel, k=2)\")\n",
    "ax[1].set_title(\"Adaptive Triangular Kernel, k=20\")\n",
    "ax[2].set_title(\"Adaptive Gaussian Kernel, k=10\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the adaptive kernels, you also obtain a distinct bandwidth at each site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wkernel_adaptive.bandwidth[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are useful in their own right, since they communicate information about the structure of the density of points in the analysis frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(8,4))\n",
    "focal_listings.assign(bandwidths=Wkernel_adaptive.bandwidth).plot('bandwidths', cmap='plasma',ax=ax[0])\n",
    "focal_listings.assign(bandwidths=Wkernel_adaptive_gaussian.bandwidth).plot('bandwidths', cmap='plasma',ax=ax[1])\n",
    "for i in range(2):\n",
    "    neighborhoods.boundary.plot(color='grey', ax=ax[i])\n",
    "    ax[i].axis(focal_listings.total_bounds[np.asarray([0,2,1,3])])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticklabels([])\n",
    "ax[0].set_title(\"Adaptive Triangular Kernel, k=20\")\n",
    "ax[0].set_ylabel(\"Site-specific bandwidths\", fontsize=16)\n",
    "ax[1].set_title(\"Adaptive Gaussian Kernel, k=10\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Areas with large adaptive kernel bandwidths are considered in \"sparse\" regions and areas with small adaptive bandwidths are in \"dense\" regions; a similar kind of logic is used by clustering algortihms descended from DBSCAN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, this is a binary kernel weight. All observations that are within a given distance from one another are considered \"neighbors,\" and all that are further than this distance are \"not neighbors.\" \n",
    "\n",
    "In order for this weighting structure to connect all observations, it's useful to set this to the largest distance connecting on observation to its nearest neighbor. This observation is the \"most remote\" observation and have at least one neighbor; every other observation is thus guaranteed to have at least this many neighbors. \n",
    "\n",
    "To get this \"m distance to the first nearest neighbor,\" you can use the PySAL `min_threshold_distance` function, which requires an array of points to find the minimum distance at which all observations are connected to at least one other observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_array = np.vstack(focal_listings.geometry.apply(lambda p: np.hstack(p.xy)))\n",
    "minthresh = lp.weights.min_threshold_distance(point_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minthresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the most remote observation is just over 171 meters away from its nearest airbnb. Building a graph from this minimum distance, then, is done by passing this to the weights constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbandW = lp.weights.DistanceBand.from_dataframe(focal_listings, threshold=minthresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods.boundary.plot(color='grey')\n",
    "dbandW.plot(focal_listings, ax=plt.gca(), edge_kws=dict(color='r'), node_kws=dict(zorder=10))\n",
    "plt.axis(focal_listings.total_bounds[np.asarray([0,2,1,3])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model of spatial relationships will guarantee that each observation has at least one neighbor, and will prevent any disconnected subgraphs from existing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNNW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$-nearest neighbor weights are constructed by considering the nearest $k$ points to each observation as neighboring that observation. This is a common way of conceptualizing observations' neighbourhoods in machine learning applications, and it is also common in geographic data science applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNW = lp.weights.KNN.from_dataframe(focal_listings, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods.boundary.plot(color='grey')\n",
    "KNNW.plot(focal_listings,ax=plt.gca(), edge_kws=dict(color='r'), node_kws=dict(zorder=10))\n",
    "plt.axis(focal_listings.total_bounds[np.asarray([0,2,1,3])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One exceedingly-common method of analysis using KNN weights is by changing `k` repeatedly and finding better values. Thus, the KNN-weights method provides a specific method to do this in a way that avoids re-constructing its core data structure, the `kdtree`. \n",
    "\n",
    "Further, this can add additional data to the weights object as well. \n",
    "\n",
    "By default, this operates in place, but can also provide a copy of the datastructure if `inplace=False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNW20 = KNNW.reweight(k=20, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods.boundary.plot(color='grey')\n",
    "KNNW20.plot(focal_listings,ax=plt.gca(), edge_kws=dict(color='r'), node_kws=dict(zorder=10))\n",
    "plt.axis(focal_listings.total_bounds[np.asarray([0,2,1,3])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, since KNN weights are asymmetric, special methods are provided to make them symmetric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNW20sym = KNNW20.symmetrize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(KNNW20sym.sparse != KNNW20sym.sparse.T).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(KNNW20.sparse != KNNW20.sparse.T).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, these symmetrizing methods exist for any other weights type too, so if you've got an arbitrarily-computed weights matrix, it can be used in that case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on Polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While K-nearest neighbors weighting methods often make more sense for data in point formats, it's also applicable to data in polygons, were a *representative point* for each polygon is used to construct K-nearest neighbors, instead of the polygons as a whole. \n",
    "\n",
    "\n",
    "For comparison, I'll show this alongside of the Queen weights shown above for neighbourhoods in Berlin. \n",
    "\n",
    "When the number of nearest neighbours is relatively large compared to the usual cardinality in an adjacency graph, this results in some neighbourhoods being connected to one another more than a single-neigbourhood deep. That is, neighbourhoods are considered spatially connected even if they don't touch, since their *representative points* are so close to one another relative to the nearest alternatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_neighborhoods = lp.weights.KNN.from_dataframe(neighborhoods, k=10).symmetrize()\n",
    "\n",
    "f,ax = plt.subplots(1,2,figsize=(8,4))\n",
    "for i in range(2):\n",
    "    neighborhoods.boundary.plot(color='grey',ax=ax[i])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticklabels([])\n",
    "KNN_neighborhoods.plot(neighborhoods, ax=ax[0], node_kws=dict(s=0), color='orangered')\n",
    "Qconnected.plot(neighborhoods, ax=ax[1], node_kws=dict(s=0), color='skyblue')\n",
    "ax[0].set_title(\"KNN(10)\", fontsize=16)\n",
    "ax[1].set_title(\"Queen Contiguity\", fontsize=16)\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conrast, very sparse K-nearest neighbours graphs will result in significantly different connectivity structure than the contiguity graph, since the relative position of large areas' *representative points* matters significantly for which observations it touches will be considered \"connected.\" Further, this often reduces the density of areas in the map with small elementary units, where cardinality is often higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_neighborhoods = lp.weights.KNN.from_dataframe(neighborhoods, k=2).symmetrize()\n",
    "\n",
    "f,ax = plt.subplots(1,2,figsize=(8,4))\n",
    "for i in range(2):\n",
    "    neighborhoods.boundary.plot(color='grey',ax=ax[i])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticklabels([])\n",
    "KNN_neighborhoods.plot(neighborhoods, ax=ax[0], node_kws=dict(s=0), color='orangered')\n",
    "Qconnected.plot(neighborhoods, ax=ax[1], node_kws=dict(s=0), color='skyblue')\n",
    "ax[0].set_title(\"KNN(2)\", fontsize=16)\n",
    "ax[1].set_title(\"Queen Contiguity\", fontsize=16)\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More representations\n",
    "\n",
    "There are similarly more representations available and currently under development, such as a networkx interface in `W.to_networkx/W.from_networkx`. Further, we're always willing to add additional constructors or methods to provide new and interesting ways to represent geographic relationships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
